= Chapter 4: Kubernetes Cluster on AWS

== 4.1 Introduction to Amazon EKS

**Slides**

. Introduction to Amazon EKS

== 4.2 Create an EKS Cluster

**Code**

This application will be deployed to an https://aws.amazon.com/eks/[Amazon EKS cluster]. Let's create the cluster first.

. Install http://eksctl.io/[eksctl] CLI:

	brew install weaveworks/tap/eksctl

. Check eksctl version:

	$ eksctl version
	[ℹ]  version.Info{BuiltAt:"", GitCommit:"", GitTag:"0.1.16"}

. Download AWS IAM Authenticator:
+
	curl -o aws-iam-authenticator https://amazon-eks.s3-us-west-2.amazonaws.com/1.11.5/2018-12-06/bin/darwin/amd64/aws-iam-authenticator
	chmod +x aws-iam-authenticator
+
Include the directory where the CLI is downloaded to your `PATH`.
+
. Create EKS cluster:

	eksctl create cluster --name k8s-course --nodes 4
	[ℹ]  using region us-west-2
	[ℹ]  setting availability zones to [us-west-2b us-west-2a us-west-2c]
	[ℹ]  subnets for us-west-2b - public:192.168.0.0/19 private:192.168.96.0/19
	[ℹ]  subnets for us-west-2a - public:192.168.32.0/19 private:192.168.128.0/19
	[ℹ]  subnets for us-west-2c - public:192.168.64.0/19 private:192.168.160.0/19
	[ℹ]  nodegroup "ng-8c5c6d93" will use "ami-094fa4044a2a3cf52" [AmazonLinux2/1.11]
	[ℹ]  creating EKS cluster "k8s-course" in "us-west-2" region
	[ℹ]  will create 2 separate CloudFormation stacks for cluster itself and the initial nodegroup
	[ℹ]  if you encounter any issues, check CloudFormation console or try 'eksctl utils describe-stacks --region=us-west-2 --name=k8s-course'
	[ℹ]  creating cluster stack "eksctl-k8s-course-cluster"
	[ℹ]  creating nodegroup stack "eksctl-k8s-course-nodegroup-ng-8c5c6d93"
	[✔]  all EKS cluster resource for "k8s-course" had been created
	[✔]  saved kubeconfig as "/Users/arungupta/.kube/config"
	[ℹ]  nodegroup "ng-8c5c6d93" has 0 node(s)
	[ℹ]  waiting for at least 4 node(s) to become ready in "ng-8c5c6d93"
	[ℹ]  nodegroup "ng-8c5c6d93" has 4 node(s)
	[ℹ]  node "ip-192-168-18-187.us-west-2.compute.internal" is ready
	[ℹ]  node "ip-192-168-19-14.us-west-2.compute.internal" is ready
	[ℹ]  node "ip-192-168-51-15.us-west-2.compute.internal" is ready
	[ℹ]  node "ip-192-168-65-119.us-west-2.compute.internal" is ready
	[ℹ]  kubectl command should work with "/Users/arungupta/.kube/config", try 'kubectl get nodes'
	[✔]  EKS cluster "k8s-course" in "us-west-2" region is ready

. Get the list of configs:
+
	kubectl config get-contexts
	CURRENT   NAME                                  CLUSTER                          AUTHINFO                              NAMESPACE
	*         arun@k8s-course.us-west-2.eksctl.io   k8s-course.us-west-2.eksctl.io   arun@k8s-course.us-west-2.eksctl.io   
	          docker-for-desktop                    docker-for-desktop-cluster       docker-for-desktop                    
	          minikube                              minikube                         minikube 
+
`*` indicates that kubectl is now configured to talk to the newly created cluster.
+
. Check the nodes:

	kubectl get nodes
	NAME                                           STATUS    ROLES     AGE       VERSION
	ip-192-168-18-187.us-west-2.compute.internal   Ready     <none>    1m        v1.11.5
	ip-192-168-19-14.us-west-2.compute.internal    Ready     <none>    1m        v1.11.5
	ip-192-168-51-15.us-west-2.compute.internal    Ready     <none>    1m        v1.11.5
	ip-192-168-65-119.us-west-2.compute.internal   Ready     <none>    1m        v1.11.5

. Print the client and server version:

	kubectl version --short
	Client Version: v1.13.1
	Server Version: v1.11.5-eks-6bad6d

. Show EKS service in the console: http://console.aws.amazon.com/console/home?region=us-west-2.

== 4.3 Migrate Application to Kubernetes Cluster on AWS

**Code**

. Explicitly set the context:

    kubectl config use-context arun@k8s-course.us-west-2.eksctl.io

. Helm's server-side component, Tiller, requires special permission on the kubernetes cluster. We need to build a Service Account for tiller to use and enable permissions for it. Then we can install Helm in EKS:

	kubectl -n kube-system create sa tiller
	kubectl create clusterrolebinding tiller --clusterrole cluster-admin --serviceaccount=kube-system:tiller
	helm init --service-account tiller

. Check the list of pods:

	kubectl get pods -n kube-system
	NAME                            READY   STATUS    RESTARTS   AGE
	aws-node-6h4r9                  1/1     Running   1          16m
	aws-node-7rwkw                  1/1     Running   1          16m
	aws-node-k9g6s                  1/1     Running   0          16m
	aws-node-t6k6v                  1/1     Running   1          16m
	kube-dns-64b69465b4-vpxjq       3/3     Running   0          23m
	kube-proxy-bnkj6                1/1     Running   0          16m
	kube-proxy-bqths                1/1     Running   0          16m
	kube-proxy-m7ctf                1/1     Running   0          16m
	kube-proxy-tszj5                1/1     Running   0          16m
	tiller-deploy-895d57dd9-zt2s2   1/1     Running   0          7s

. Redeploy the application:

	helm install --name myapp manifests/charts/myapp

. Get the service:
+
	kubectl get svc
	NAME         TYPE           CLUSTER-IP       EXTERNAL-IP                                                              PORT(S)                         AGE
	greeting     LoadBalancer   10.100.110.146   a798a47cd104211e986290adde103247-521253658.us-west-2.elb.amazonaws.com   8080:31627/TCP,5005:30216/TCP   2m
	kubernetes   ClusterIP      10.100.0.1       <none>                                                                   443/TCP                         18h
+
It shows the port `8080` and `5005` are published and an Elastic Load Balancer is provisioned. It takes about three minutes for the load balancer to be ready.
+
. Access the application:

	curl http://$(kubectl get svc/greeting \
		-o jsonpath='{.status.loadBalancer.ingress[0].hostname}'):8080/hello

. Delete the application:

	helm delete --purge myapp

== 4.3 Migrate Application to Kubernetes Cluster on AWS with Helm version 3

**Code**

. Explicitly set the context:

    kubectl config use-context arun@k8s-course.us-west-2.eksctl.io

. Check the list of pods:

	kubectl get pods -n kube-system
	NAME                            READY   STATUS    RESTARTS   AGE
	aws-node-6h4r9                  1/1     Running   1          16m
	aws-node-7rwkw                  1/1     Running   1          16m
	aws-node-k9g6s                  1/1     Running   0          16m
	aws-node-t6k6v                  1/1     Running   1          16m
	kube-dns-64b69465b4-vpxjq       3/3     Running   0          23m
	kube-proxy-bnkj6                1/1     Running   0          16m
	kube-proxy-bqths                1/1     Running   0          16m
	kube-proxy-m7ctf                1/1     Running   0          16m
	kube-proxy-tszj5                1/1     Running   0          16m
	tiller-deploy-895d57dd9-zt2s2   1/1     Running   0          7s

. Redeploy the application:

	helm install myapp manifests/charts/myapp

. Get the service:
+
	kubectl get svc
	NAME         TYPE           CLUSTER-IP       EXTERNAL-IP                                                              PORT(S)                         AGE
	greeting     LoadBalancer   10.100.110.146   a798a47cd104211e986290adde103247-521253658.us-west-2.elb.amazonaws.com   8080:31627/TCP,5005:30216/TCP   2m
	kubernetes   ClusterIP      10.100.0.1       <none>                                                                   443/TCP                         18h
+
It shows the port `8080` and `5005` are published and an Elastic Load Balancer is provisioned. It takes about three minutes for the load balancer to be ready.
+
. Access the application:

	curl http://$(kubectl get svc/greeting \
		-o jsonpath='{.status.loadBalancer.ingress[0].hostname}'):8080/hello

. Delete the application:

	helm uninstall myapp
